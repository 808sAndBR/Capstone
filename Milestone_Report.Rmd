---
title: "Milestone Report"
author: "Scott Brenstuhl"
date: "March 12, 2016"
output: html_document
---

```{r, echo=FALSE, alert=FALSE, message = FALSE}
library(ggplot2)
library(tm)
#library(quanteda)
library(RWeka)
library(stringi)
library(wordcloud)
```


```{r, echo=FALSE, alert=FALSE, message = FALSE}
en_twit <- readLines('final/en_US/en_US.twitter.txt')
en_blog <- readLines('final/en_US/en_US.blogs.txt')
en_news <- readLines('final/en_US/en_US.news.txt')

twit_sample <- readLines('sample/twitter_sample.txt')
blog_sample <- readLines('sample/blog_sample.txt')
news_sample <- readLines('sample/news_sample.txt')
```

## Summary

Using the bodies of text samples provided by Coursera I am builind an app to
predict what the next word will be when the begining of a phrase is entered. In
this exploritory analysis I will share information about the data, my initial 
findings and my plans of how to move forward with the app.

## The Data

Coursera provides all of the data here:</br>
https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip
</br>
I am only taking the en_US subdirectory into consideration. If we look at all the 
data we see that it is massive:

```{r echo = FALSE}
# Return summary stats
text.stats <- function(text){
    rows.count = length(text)
    words.count = sum(stri_count_words(text))
    paste(rows.count, 'rows and', words.count, "words.")
}

paste("The Twitter data has",
    text.stats(en_twit))
paste("The blog data has",
    text.stats(en_blog))
paste("The news data has",
    text.stats(en_news))

# Remove the big stuff that we aren't going to use anymore.
rm(list = list(en_twit, en_blog, en_news))

```

Since these are so massive, I took a 1% sample of each for exploration. This
brings the size of our data to a lot more managable size:

```{r echo = FALSE}
paste("The Twitter sample has",
    text.stats(en_twit))
paste("The blog sample has",
    text.stats(en_blog))
paste("The news sample has",
    text.stats(en_news))
```

```{r echo = FALSE}
# Pulling great list of dirty words to be removed
dirty.words.link <- 'https://raw.githubusercontent.com/shutterstock/List-of-Dirty-Naughty-Obscene-and-Otherwise-Bad-Words/master/en'
dirty.words <- readLines(dirty.words.link)
dirty.words <- paste0("\\b(",paste0(dirty.words,collapse="|"), ")\\b") 


text.cleaner <- function(text.body, remove.profaity = TRUE){
    # Gets rid of all the dirty words
    if(remove.profaity){
        clean.text = gsub(dirty.words, ' ', text.body)
    }else{
        clean.text = text.body
    }
    # This section is mostly aimed at twitter but will clean up
    # references to twitter things in other sources.
    clean.text = gsub('(@\\S*)', ' ', clean.text)
    clean.text = gsub('(#\\S*)', ' ', clean.text)
    clean.text = gsub('\\bRT\\b', ' ', clean.text)
    # Regex was really figthing me when I tried to get RT: in one go
    clean.text = gsub(':', ' ', clean.text)
    # Maybe remove duplicate tweets at this point
    clean.text = iconv(clean.text, to="ASCII", sub=" ")
}

```

```{r cache = TRUE, echo = FALSE}
# Prep the data for corpus
sourcify <- function(text.body, clean = TRUE, remove.profanity = TRUE){
    if(clean){
        text.body = text.cleaner(text.body, remove.profanity)
    }
    VectorSource(text.body)
}


corp_sources <- VectorSource(lapply(list(blog_sample,
                                  twit_sample,
                                  news_sample), sourcify))

corp <- Corpus(corp_sources, readerControl = list(language="en"))
summary(corp)

corp <- tm_map(corp, removeNumbers)
corp <- tm_map(corp, removePunctuation)
corp <- tm_map(corp, tolower)
# Maybe dont do this when predicting
corp <- tm_map(corp, stemDocument, language = "english")
corp <- tm_map(corp, stripWhitespace)
corp <- tm_map(corp, PlainTextDocument)

sw.removed.corp <- tm_map(corp, removeWords, stopwords("english"))
sw.removed.corp <- tm_map(sw.removed.corp, stripWhitespace)
sw.removed.corp <- tm_map(sw.removed.corp, PlainTextDocument)

dtm <- DocumentTermMatrix(corp) 
sw.removed.dtm <- DocumentTermMatrix(sw.removed.corp)

ngramify =function(x) {NGramTokenizer(x, Weka_control(min=2, max=4))}

#super slow should cache
options(mc.cores=1)
ngram.dtm <- DocumentTermMatrix(corp, control = list(tokenize = ngramify))
sw.removed.ngram.dtm <- DocumentTermMatrix(sw.removed.corp,
                                           control = list(tokenize = ngramify))

# Explore
# inspect(corp[1])[[1]][1]
```

```{r echo = FALSE}
freq <- colSums(as.matrix(dtm))
swr.freq <- colSums(as.matrix(sw.removed.dtm))
length(freq)

# shoudlnt be own var sort(freq) makes more sense maybe?
ord <- order(freq)

hist(freq)
hist(freq[freq>10])
hist(freq[freq>100])
hist(freq[freq>1000])

freq.table = sort(table(freq))

# Would be cool to make gif of below getting higer and higher
# freq.table = freq.table[as.numeric(names(freq.table))>8]
# freq.table = freq.table[freq.table > 8]

qplot(as.numeric(names(freq.table)), freq.table,
      xlab= 'Appearances in Corpus', ylab = '# of Words with x Appearences')

dtms <- removeSparseTerms(dtm, 0.1) 

head(sort(freq, decreasing = TRUE), 200)
head(sort(swr.freq, decreasing = TRUE), 200)
# Random inspections
#inspect(dtms[, 5000:5020])
over.100 <- (freq[freq>100])
wordcloud(names(over.100), over.100, max.words=100, colors=brewer.pal(6, "Dark2"))
swr.over.100 <- (swr.freq[swr.freq>100])
wordcloud(names(swr.over.100), swr.over.100, max.words=100, colors=brewer.pal(6, "Dark2"))



nfreq <- colSums(as.matrix(ngram.dtm))
head(sort(nfreq, decreasing = TRUE), 200)
wordcloud(names(nfreq), nfreq, max.words=100, colors=brewer.pal(6, "Dark2"))
#ngram.dtm

nfreq.table = sort(table(nfreq))
qplot(as.numeric(names(nfreq.table)), nfreq.table,
      xlab= 'Appearances in Corpus', ylab = '# of Phrases with x Appearences')
```

## n-grams

```{r echo = FALSE}
swr.ngram.freq <- colSums(as.matrix(sw.removed.ngram.dtm))
head(sort(swr.ngram.freq, decreasing = TRUE), 200)
wordcloud(names(swr.ngram.freq), swr.ngram.freq, max.words=100, colors=brewer.pal(6, "Dark2"))

hist(swr.ngram.freq[swr.ngram.freq>20])

```

Removing punctuation is doing apostrophies in a weird way, breaking won't etc.

most popular bi and tri grams by source
Check for repetative long n-grams (are likelty bad to keep if boilerplate)
Remove unix things

mycorpusname[[3]]$content[20]
